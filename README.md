# Boaz-Paper
ğŸ“Œ BOAZ ë…¼ë¬¸ìŠ¤í„°ë”” (NLP2ì¡°)

| ë‚ ì§œ 	| ë…¼ë¬¸ëª… 	| ë°œí‘œìë£Œ 	| ë°œí‘œì 	|
|-	|-	|-	|-	|
| 03/18	| [Enriching Word Vectors with Subword Information](https://arxiv.org/abs/1607.04606)	| [FastText](https://github.com/hello-im-yj/Boaz-Paper/blob/main/PPT/FastText.pdf) 	| ê¹€ìœ ì§„ 	|
| 04/01 	| [Deep contextualized word representations](https://arxiv.org/abs/1802.05365 ) 	| [ELMo](https://github.com/hello-im-yj/Boaz-Paper/blob/main/PPT/ELMo.pdf) 	| ì†¡ê²½ë¯¼ 	|
| 04/08 	| [Attention Is All You Need](https://arxiv.org/abs/1706.03762 ) 	| [Transformer](https://github.com/hello-im-yj/Boaz-Paper/blob/main/PPT/Transformer.pdf)	| ì´ìƒë¯¼ 	|
| 04/15 	| [Improving Language Understanding by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) 	| [GPT](https://github.com/hello-im-yj/Boaz-Paper/blob/main/PPT/GPT-1.pdf) 	| ë¬¸ì˜ˆì§„ 	|
| 05/13 	| [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/pdf/1906.08237v2.pdf) 	| [XLNet](https://github.com/hello-im-yj/Boaz-Paper/blob/main/PPT/XLNet.pdf) 	| ê¹€ìœ ì§„ 	|
| 05/20 	| [Multi-Task Deep Neural Networks for Natural Language Understanding](https://arxiv.org/pdf/1901.11504.pdf) 	| [MT-DNN](https://github.com/hello-im-yj/Boaz-Paper/blob/main/PPT/MT-DNN.pdf) 	| í•œìœ ê²½ 	|
